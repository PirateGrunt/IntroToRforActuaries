---
author: "Brian A. Fannin"
output: slidy_presentation
duration: 30
fig_height: 4
fig_width: 10
title: "Advanced Modeling"
---

# Continuous vs. Grouped data

Continuous data is fairly easy to model. Integral data which can take many values, e.g. Poisson, may also model well. However, actuaries often work with grouped or categorical data. We can map these to integers, but the numbers convey no meaning.

* State
* Agency
* Line of business

# Quick model

```{r }
library(ggplot2)
set.seed(1234)
numGroups <- 5
numClaims <- 10
N <- numClaims * numGroups
x <- rnorm(N, 1000, 300)
link <- 1.5

groupVals <- rnorm(numGroups, mean = link, sd = .05 * link)
names(groupVals) <- head(letters, numGroups)
links <- sapply(groupVals, function(x){
  rnorm(numClaims, x, sd = .3 * x)
})

group <- replicate(numClaims, names(groupVals))
group <- as.vector(t(group))

links <- as.vector(links)
df <- data.frame(Group = group
                 , Link = link
                 , SampleLink = links
                 , x = x
                 , stringsAsFactors = FALSE)

df$y <- df$x * df$SampleLink
```


# Look at our data

```{r }
plt <- ggplot(df, aes(x, y, color = Group)) + geom_point() 
plt + stat_smooth(method = "lm", fullrange = TRUE, se = FALSE)
```

# Fit our data using `lm`

```{r }
fit1 <- lm(y ~ x + group, data = df)
summary(fit1)
```

Hang on, we don't want an intercept.

# Remove intercept

```{r }
fit2 <- lm(y ~ 0 + x + group, data = df)
summary(fit2)
```

So, my residuals didn't change, x didn't change and those group parameters look an awful lot like intercepts.

# Group data without intercept

```{r }
fitIndividual <- lm(y ~ 0 + x:group, data = df)
summary(fitIndividual)
```

Now, you're talking!

But wait, I have `r N` data points. Am I using all of them for each group?

# One group at a time

```{r }
lstSplit <- split(df, df$Group)
fits <- lapply(lstSplit, function(z){
  fit <- lm(y ~ 0 + x, data = z)
  fit
})

sapply(fits, coef)
coef(fitIndividual)
```

Each group is modelled separately!

# Grouped data is looped data

Using grouped information is like playing a piano. There are no notes between the keys.

Using a hierarchical model is like slide guitar.

# `nlme`

```{r}
library(nlme)
fitBlended <- lme(data = df, fixed = y ~ 0 + x, random = ~ 0 + x | group)
summary(fitBlended)
unlist(coef(fitBlended))
coef(fitIndividual)
```

```{r }
sum(df$y - predict(fitBlended))
sum(df$y - predict(fitIndividual))
```

Apparent inferior fit is _by design_. It indicates less than full credibility.

```{r}
sum(df$y - link * df$x)
```

Our **best** predictor would be the global parameter, if we knew what it was. Notice that all of the link ratios in the hierarchical model are closer to 1.5. They will deal better with out of sample data.

# Hierarchical modelling = credibility

Hierarchical modeling allows us to hedge our bets. If the group parameters are close together, we'll blend all of our experience. If they're far apart, each group stands on its own. 

In the simulation the variance of hypothetical means - the variability between the groups- was less than the expected value of process variance - the variability of the simulated link ratios.

Anyone remember this?

$$Z=P/(P+K)$$
$$K=EVPV/VHM$$

EVPV = Expected Value of the process variance

VHM = Variance of the hypothetical means

# Both models with our data

```{r }
df$IndividualPrediction <- predict(fitIndividual)
df$BlendedPrediction <- predict(fitBlended)

plt <- ggplot(df, aes(x, IndividualPrediction, color = group)) + geom_line() + geom_point(aes(y = y))
plt + geom_line(aes(y = BlendedPrediction), linetype = "dotted")
```

# Pooled data

```{r}
fitPooled <- lm(y ~ 0 + x, data = df)
summary(fitPooled)
```

# 

```{r}
df$PooledPrediction <- predict(fitPooled)
plt <- ggplot(df, aes(x, IndividualPrediction, color = group)) + geom_line() + geom_point(aes(y = y))
plt <- plt + geom_line(aes(y = BlendedPrediction), linetype = "dotted")
plt <- plt + geom_line(aes(y = PooledPrediction), color = "Black")
plt
```

# How about Poisson?

$$\frac{\theta^ke^{-\theta}}{k!}$$

We can get away with this if $\theta$ is actually equal to a parameter, $\lambda$ times some exposure amount. This will automatically scale to all of our loss amounts. Here's the probability distribution for our first observation 

```{r }
sampleX <- df[1, "x"]
sampleY <- df[1, "y"]
xLims <- sampleX + c(-100, 100)
xLims <- as.integer(1.5 * xLims)
pltPoisson <- ggplot(data.frame(x = xLims), aes(x)) 
pltPoisson <- pltPoisson + stat_function(fun = dpois, args = list(lambda = sampleX * 1.5))
pltPoisson <- pltPoisson + geom_vline(xintercept = sampleY, color = "red")
pltPoisson
```

This doesn't look too bad for one single point. Maybe our lambda just needs a bit of adjustment. So how do we estimate lambda?

# Enter Bayesian estimation

$$f(\theta|data)=\frac{f(data|\theta)f(\theta)}{f(data)}$$

We'll use data to estimate our parameter and then we'll use our parameter to estimate our data, which will produce another parameter and so on until we open a hole in the space-time continuum. 

The following example has been modified by a similar example from "Bayesian Computation with R" by Jim Albert.

# Assume that lambda has a gamma distribution

The fun thing about Bayesian analysis is that we don't worry so much abouut the math.

$$f(\lambda)=\frac{1}{\Gamma(\alpha)\beta^{-k}}$$

```{r }
xLims <- c(0, 2)
pltPrior <- ggplot(data.frame(x = xLims), aes(x)) 
pltPrior <- pltPrior + stat_function(fun = function(x) {1/x})
pltPrior
```


# Mama we're all Bayesy now!

# Uninformed prior

We'll use a gamma for the lambda parameter of our Poisson. An uninfomred prior 

```{r}
xLims <- c(0, 3)
pltPrior <- ggplot(data.frame(x = xLims), aes(x)) 
pltPrior <- pltPrior + stat_function(fun = dgamma, args = list(shape = 1, rate = 1))
pltPrior
```

# Posterior lambda

```{r }
pooledLambda <- sum(df$y) / sum(df$x)

pooledAlpha <- sum(df$y)
pooledBeta <- sum(df$x) 

xLims <- c(1.4, 1.6)
pltPooled <- ggplot(data.frame(x = xLims), aes(x)) 
pltPooled <- pltPooled + stat_function(fun = dgamma, args = list(shape = pooledAlpha, rate = pooledBeta), n = 500)
pltPooled

xLims <- c(1.4, 1.6)
pltPooled <- ggplot(data.frame(x = xLims), aes(x)) 
pltPooled <- pltPooled + stat_function(fun = dgamma, args = list(shape = pooledAlpha, rate = pooledBeta), n = 500)
pltPooled
```

# Single group lambda

```{r }
alphaA <- sum(df$y[df$Group == "a"])
betaA <- sum(df$x[df$Group == "a"])
lambdaA <- alphaA / betaA

pltGroupA <- ggplot(data.frame(x = xLims), aes(x)) 
pltGroupA <- pltGroupA + stat_function(fun = dgamma
                                       , n = 500
                                       , colour = "red"
                                       , args = list(shape = pooledAlpha + alphaA, rate = pooledBeta + betaA))
pltGroupA

pltGroupA <- pltGroupA + stat_function(fun = dgamma
                                       , n = 500
                                       , colour = "black"
                                       , args = list(shape = pooledAlpha, rate = pooledBeta))
pltGroupA
```

# Posterior prediction

```{r }
sampleLambda <- rgamma(1000, alphaA, betaA)
sampleX <- sample(df$x[df$Group == "a"], 1000, replace = TRUE)
sampleY <- rpois(1000, sampleLambda * sampleX)
hist(sampleY)
hist(df$y[df$Group == "a"])
```


# All groups

```{r }
lstSplit <- split(df, df$Group)
groupedLambda <- sapply(lstSplit, function(z){
  lambda <- sum(z$y) / sum(z$x)
})

```

# Real data

```{r }
library(raw)
data("ppauto")
```

# Fun stuff to read:

* Gelman and Hill
* Guszcza's hierarchical growth model paper
